# file: UNSW_NB15_processed_llm_smote_excel_custom_folder.py
# ç”¨é€”ï¼šå¤„ç† UNSW-NB15 æ•°æ®ï¼Œä½¿ç”¨ SMOTEï¼Œè½¬æ¢ä¸º JSONLï¼Œå¹¶ç”Ÿæˆ Excel æŠ¥å‘Šåˆ°è‡ªå®šä¹‰æ–‡ä»¶å¤¹

from tqdm import tqdm
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import logging
import json
import os
from collections import Counter
from imblearn.over_sampling import SMOTE

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def save_distribution_to_excel(train_dist, test_dist, label_map, output_path):
    """
    å°†è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ•°æ®åˆ†å¸ƒæƒ…å†µä¿å­˜åˆ° Excel æ–‡ä»¶ä¸­ã€‚
    """
    id_to_label = {v: k for k, v in label_map.items()}
    all_labels = sorted(label_map.values())

    data_for_df = []
    for label_id in all_labels:
        data_for_df.append({
            'Category': id_to_label[label_id],
            'Label ID': label_id,
            'Train Count': train_dist.get(label_id, 0),
            'Test Count': test_dist.get(label_id, 0)
        })

    df = pd.DataFrame(data_for_df)
    
    total_row = {
        'Category': 'TOTAL',
        'Label ID': '',
        'Train Count': df['Train Count'].sum(),
        'Test Count': df['Test Count'].sum()
    }
    df = pd.concat([df, pd.DataFrame([total_row])], ignore_index=True)

    try:
        df.to_excel(output_path, index=False, engine='openpyxl')
        logging.info(f"ğŸ’¾ Successfully saved data distribution report to {output_path}")
    except Exception as e:
        logging.error(f"âŒ Failed to save Excel file {output_path}. Error: {e}")


def convert_to_text_and_save(df, label_series, output_path, task_type='classification'):
    """
    å°†æ•°å€¼å’Œåˆ†ç±»æ•°æ®è½¬æ¢ä¸ºè‡ªç„¶æ–‡æœ¬æ ¼å¼å¹¶ä¿å­˜
    """
    logging.info(f"Converting data to natural text format for task '{task_type}' and saving to {output_path}")
    
    lines = []
    prompt_template = "Network flow features: "
    label_series_list = label_series.tolist()

    for i in tqdm(range(len(df)), desc=f"Generating text for {os.path.basename(output_path)}"):
        row = df.iloc[i]
        feature_parts = []
        non_zero_features = row[row.abs() > 0.1]
        
        if len(non_zero_features) > 25:
            selected_features = non_zero_features.sample(n=25, random_state=i)
        else:
            selected_features = non_zero_features

        for feature, value in selected_features.items():
            feature_parts.append(f"{feature.replace('_', ' ')} is {value:.2f}")
            
        feature_str = "; ".join(feature_parts)
        text_content = prompt_template + feature_str

        if task_type == 'classification':
            record = {'text': text_content, 'label': int(label_series_list[i])}
            lines.append(json.dumps(record, ensure_ascii=False))

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    logging.info(f"Successfully saved {len(lines)} lines to {output_path}")


def process_and_save_data(filepath, output_folder, debug_rows=None, task_type='classification'):
    """
    ä¸»æ•°æ®å¤„ç†å‡½æ•°
    """
    logging.info(f"Starting data loading from: {filepath}")
    full_df = pd.read_csv(filepath, encoding='latin1', header=0, low_memory=False)
    
    # ... (åŸºæœ¬é¢„å¤„ç†å’Œæ ‡ç­¾å¤„ç†éƒ¨åˆ†ä¿æŒä¸å˜) ...
    full_df.columns = [col.strip().lower() for col in full_df.columns]
    if 'id' in full_df.columns: full_df = full_df.drop('id', axis=1)

    label_col = 'label'
    if 'attack_cat' in full_df.columns:
        full_df['attack_cat'] = full_df['attack_cat'].str.strip()
        unique_categories = sorted(full_df['attack_cat'].unique())
        if 'Normal' in unique_categories:
            unique_categories.remove('Normal')
            unique_categories.insert(0, 'Normal')
        label_map = {cat: i for i, cat in enumerate(unique_categories)}
        num_classes = len(label_map)
        full_df[label_col] = full_df['attack_cat'].map(label_map)
        full_df = full_df.drop('attack_cat', axis=1)
        logging.info("="*60 + "\nğŸ“Š æ ‡ç­¾æ˜ å°„å®Œæˆ\n" + "="*60)
    else:
        logging.error("âŒ æœªæ‰¾åˆ° 'attack_cat' åˆ—ã€‚")
        return None

    # ... (å¤„ç†æ··åˆç±»å‹ã€ç¼ºå¤±å€¼ã€Debugæ¨¡å¼éƒ¨åˆ†ä¿æŒä¸å˜) ...
    for col in full_df.select_dtypes(include=['object']).columns:
        if col not in ['proto', 'service', 'state']:
            full_df[col] = pd.to_numeric(full_df[col], errors='coerce')
    full_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    full_df = full_df.dropna(subset=[label_col])
    
    if debug_rows:
        logging.info(f"Debug mode: Using {debug_rows} rows")
        _, df = train_test_split(full_df, train_size=debug_rows, stratify=full_df[label_col], random_state=42)
    else:
        df = full_df
        
    y = df.pop(label_col)
    X = df
    
    # ... (æ•°å€¼å¤„ç†ã€ç¼–ç ã€å½’ä¸€åŒ–éƒ¨åˆ†ä¿æŒä¸å˜) ...
    X = pd.get_dummies(X, columns=['proto', 'service', 'state'])
    for col in X.select_dtypes(include=np.number).columns:
        if X[col].isnull().any(): X[col].fillna(X[col].mean(), inplace=True)
    scaler = StandardScaler()
    numerical_cols = X.select_dtypes(include=np.number).columns.tolist()
    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    # --- [ä¿®æ”¹] ---
    # åˆ›å»ºæŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶å¤¹
    os.makedirs(output_folder, exist_ok=True)
    
    # ä¿å­˜ SMOTE å‰çš„æ•°æ®åˆ†å¸ƒåˆ° Excel
    save_distribution_to_excel(
        y_train.value_counts().to_dict(),
        y_test.value_counts().to_dict(),
        label_map,
        os.path.join(output_folder, 'data_distribution_before_smote.xlsx')
    )
    
    # ä½¿ç”¨ SMOTE å¤„ç†è®­ç»ƒé›†
    logging.info("="*60 + "\nâš–ï¸ å¼€å§‹å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ (SMOTE)...\n" + "="*60)
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    # ä¿å­˜ SMOTE åçš„æ•°æ®åˆ†å¸ƒåˆ° Excel
    save_distribution_to_excel(
        pd.Series(y_train_resampled).value_counts().to_dict(),
        y_test.value_counts().to_dict(),
        label_map,
        os.path.join(output_folder, 'data_distribution_after_smote.xlsx')
    )
    
    # --- [ä¿®æ”¹] ---
    # ç”Ÿæˆå¹¶ä¿å­˜æ–‡æœ¬æ–‡ä»¶åˆ°æŒ‡å®šæ–‡ä»¶å¤¹
    convert_to_text_and_save(X_train_resampled, pd.Series(y_train_resampled), os.path.join(output_folder, 'train_data.jsonl'), task_type)
    convert_to_text_and_save(X_test, y_test, os.path.join(output_folder, 'test_data.jsonl'), task_type)

    logging.info("âœ… Data pre-processing and natural text conversion complete!")
    
    # ... (è¿”å›ç»Ÿè®¡ä¿¡æ¯éƒ¨åˆ†ä¿æŒä¸å˜) ...
    stats = {
        'num_classes': num_classes, 'label_map': label_map,
        'train_size_before_smote': len(X_train), 'train_size_after_smote': len(X_train_resampled),
        'test_size': len(X_test), 'train_distribution_before_smote': y_train.value_counts().to_dict(),
        'train_distribution_after_smote': pd.Series(y_train_resampled).value_counts().to_dict(),
        'test_distribution': y_test.value_counts().to_dict()
    }
    return stats


def check_and_prepare_data(data_path, output_folder, debug_rows=None, force_reprocess=False):
    """
    æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨å¤„ç†
    """
    # --- [ä¿®æ”¹] ---
    # æ›´æ–°æ£€æŸ¥æ–‡ä»¶çš„è·¯å¾„
    train_file = os.path.join(output_folder, 'train_data.jsonl')
    test_file = os.path.join(output_folder, 'test_data.jsonl')
    
    if os.path.exists(train_file) and os.path.exists(test_file) and not force_reprocess:
        logging.info(f"âœ… å‘ç°å·²å¤„ç†çš„æ•°æ®æ–‡ä»¶äº '{output_folder}'ï¼Œè·³è¿‡å¤„ç†ã€‚")
        return True
    
    if force_reprocess: logging.warning("âš ï¸  å¼ºåˆ¶é‡æ–°å¤„ç†æ•°æ®...")
    else: logging.warning(f"âš ï¸  æœªæ‰¾åˆ°å·²å¤„ç†çš„æ•°æ®æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†åŸå§‹æ•°æ®è‡³ '{output_folder}'...")
    
    if not os.path.exists(data_path):
        logging.error(f"âŒ åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return False
    
    try:
        # --- [ä¿®æ”¹] ---
        # å°† output_folder ä¼ é€’ç»™ä¸»å¤„ç†å‡½æ•°
        stats = process_and_save_data(data_path, output_folder, debug_rows=debug_rows)
        
        # ... (æ—¥å¿—æ‰“å°éƒ¨åˆ†ä¿æŒä¸å˜) ...
        if stats:
            logging.info("\n" + "="*60)
            logging.info("ğŸ“Š æ•°æ®å¤„ç†ç»Ÿè®¡ (å¤šåˆ†ç±», SMOTE):")
            logging.info(f"  è®­ç»ƒé›† (SMOTEå): {stats['train_size_after_smote']} æ¡ (åŸä¸º {stats['train_size_before_smote']} æ¡)")
            logging.info(f"  æµ‹è¯•é›†: {stats['test_size']} æ¡ (ä¿æŒä¸å˜)")
            logging.info("="*60 + "\n")
        return True
    except Exception as e:
        logging.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    data_path = 'data/UNSW-NB15.csv'
    # --- [ä¿®æ”¹] ---
    # å®šä¹‰æ–°çš„è¾“å‡ºæ–‡ä»¶å¤¹
    output_folder = 'processed_data_SMOTE'
    debug_rows = 20000 
    
    print("="*60)
    print(f"UNSW-NB15 æ•°æ®å¤„ç†è„šæœ¬ (è¾“å‡ºè‡³: {output_folder})")
    print("="*60 + "\n")
    
    success = check_and_prepare_data(
        data_path=data_path,
        output_folder=output_folder, # ä¼ é€’æ–‡ä»¶å¤¹åç§°
        debug_rows=debug_rows,
        force_reprocess=True 
    )
    
    if success:
        print("\nâœ… æ•°æ®å‡†å¤‡å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚")
        # --- [ä¿®æ”¹] ---
        # æ›´æ–°æœ€ç»ˆæç¤ºä¿¡æ¯
        print(f"ğŸ“Š JSONL æ•°æ®å’Œ Excel æŠ¥å‘Šå·²ç”Ÿæˆäº '{output_folder}' æ–‡ä»¶å¤¹ä¸­ã€‚")
    else:
        print("\nâŒ æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
