# file: sfl_sanity_check.py
# ç”¨é€”ï¼šåˆ†å‰²è”é‚¦å­¦ä¹ ï¼ˆSFLï¼‰ä¸»è®­ç»ƒè„šæœ¬ï¼Œä½¿ç”¨ LoRA å¾®è°ƒ RoBERTa æ¨¡å‹
# æ”¹è¿›ç‰ˆï¼šå¢å¼ºè®­ç»ƒå¼ºåº¦ + ç±»åˆ«æƒé‡ + è¯¦ç»†è¯„ä¼°

import torch
from torch import nn
from torch.utils.data import DataLoader, Subset
from transformers import RobertaForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW
from peft import LoraConfig, get_peft_model, TaskType
from tqdm import tqdm
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import logging
import time
import csv
import random
from collections import OrderedDict, Counter
import copy
import os

from data_utils import FT_Dataset, get_tokenizer

# ============================== é…ç½® ==============================
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s', 
    datefmt='%Y-%m-%d %H:%M:%S'
)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ==================== æ¨¡å¼åˆ‡æ¢ï¼ˆå¢å¼ºç‰ˆï¼‰====================
MODE = 'QUICK'  # 'QUICK' ç”¨äºå¿«é€Ÿé€»è¾‘æ£€æŸ¥, 'NORMAL' ç”¨äºå®Œæ•´è®­ç»ƒ

if MODE == 'QUICK':
    logging.warning("<<<<< æ­£åœ¨ä»¥ SFL QUICK æ¨¡å¼è¿è¡Œ >>>>>")
    # â¬‡ï¸â¬‡ï¸ å¢å¼ºè®­ç»ƒï¼š10è½®ï¼Œæ¯è½®5ä¸ªepoch â¬‡ï¸â¬‡ï¸
    ROUNDS, NUM_CLIENTS, CLIENTS_PER_ROUND, LOCAL_EPOCHS, DEBUG_DATA_SIZE, SPLIT_LAYER = 10, 4, 2, 5, 2000, 4
else:
    logging.info(">>>>> æ­£åœ¨ä»¥ SFL NORMAL æ¨¡å¼è¿è¡Œ <<<<<")
    ROUNDS, NUM_CLIENTS, CLIENTS_PER_ROUND, LOCAL_EPOCHS, DEBUG_DATA_SIZE, SPLIT_LAYER = 50, 20, 5, 5, None, 4

# ==================== ä½¿ç”¨æœ¬åœ°æ¨¡å‹ ====================
MODEL_NAME = './roberta-base-local'
MODEL_NAME_ON_HUB = 'roberta-base'

# â¬‡ï¸â¬‡ï¸ æé«˜å­¦ä¹ ç‡åˆ° 1e-4ï¼ˆåŸæ¥æ˜¯ 2e-5ï¼‰â¬‡ï¸â¬‡ï¸
LR, BATCH_SIZE, MAX_SEQ_LENGTH = 1e-4, 16, 128
LORA_R, LORA_ALPHA, LORA_DROPOUT = 16, 32, 0.1
RESULTS_FILENAME = f"SFL_{MODE.lower()}_R{ROUNDS}_C{NUM_CLIENTS}_{time.strftime('%Y%m%d_%H%M%S')}.csv"

# ============================== SFL æ¨¡å‹å®šä¹‰ ==============================
class ClientModelSFL(nn.Module):
    """å®¢æˆ·ç«¯æ¨¡å‹ï¼šåŒ…å«åµŒå…¥å±‚å’Œå‰ N å±‚ç¼–ç å™¨"""
    def __init__(self, full_model, split_layer):
        super().__init__()
        base_model = full_model.base_model.model if hasattr(full_model, 'base_model') else full_model
        self.embeddings = base_model.roberta.embeddings
        self.encoder_layers = base_model.roberta.encoder.layer[:split_layer]
    
    def forward(self, input_ids, attention_mask=None):
        if attention_mask is None: 
            attention_mask = torch.ones_like(input_ids)
        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        extended_attention_mask = extended_attention_mask.to(dtype=self.embeddings.word_embeddings.weight.dtype)
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        hidden_states = self.embeddings(input_ids=input_ids)
        for layer in self.encoder_layers: 
            hidden_states = layer(hidden_states, attention_mask=extended_attention_mask)[0]
        return hidden_states, extended_attention_mask


class ServerModelSFL(nn.Module):
    """æœåŠ¡å™¨ç«¯æ¨¡å‹ï¼šåŒ…å«å‰©ä½™ç¼–ç å™¨å±‚å’Œåˆ†ç±»å¤´ï¼ˆæ”¯æŒç±»åˆ«æƒé‡ï¼‰"""
    def __init__(self, full_model, split_layer):
        super().__init__()
        base_model = full_model.base_model.model if hasattr(full_model, 'base_model') else full_model
        self.encoder_layers = base_model.roberta.encoder.layer[split_layer:]
        self.classifier = base_model.classifier
    
    def forward(self, hidden_states, attention_mask, labels=None, class_weights=None):
        """
        å‰å‘ä¼ æ’­ï¼Œæ”¯æŒç±»åˆ«æƒé‡
        
        Args:
            hidden_states: éšè—çŠ¶æ€
            attention_mask: æ³¨æ„åŠ›æ©ç 
            labels: æ ‡ç­¾
            class_weights: ç±»åˆ«æƒé‡ï¼ˆç”¨äºå¤„ç†ä¸å¹³è¡¡æ•°æ®ï¼‰
        """
        for layer in self.encoder_layers: 
            hidden_states = layer(hidden_states, attention_mask=attention_mask)[0]
        logits = self.classifier(hidden_states)
        loss = None
        if labels is not None:
            # â¬‡ï¸â¬‡ï¸ ä½¿ç”¨ç±»åˆ«æƒé‡ â¬‡ï¸â¬‡ï¸
            if class_weights is not None:
                loss_fct = nn.CrossEntropyLoss(weight=class_weights)
            else:
                loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, 2), labels.view(-1))
        return logits, loss


def create_and_split_model(split_layer, model_path):
    """åˆ›å»ºå¹¶åˆ†å‰²æ¨¡å‹"""
    logging.info(f"æ­£åœ¨ä»è·¯å¾„ '{model_path}' åŠ è½½æ¨¡å‹...")
    logging.info(f"åˆ†å‰²å±‚è®¾ç½®ä¸º: {split_layer}")
    
    full_model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=2)
    
    peft_config = LoraConfig(
        task_type=TaskType.SEQ_CLS, 
        r=LORA_R, 
        lora_alpha=LORA_ALPHA, 
        lora_dropout=LORA_DROPOUT, 
        target_modules=["query", "key", "value"], 
        modules_to_save=["classifier"]
    )
    full_model_peft = get_peft_model(full_model, peft_config)
    
    logging.info("--- åº”ç”¨ PEFT åçš„å®Œæ•´æ¨¡å‹å¯è®­ç»ƒå‚æ•° ---")
    full_model_peft.print_trainable_parameters()
    
    client_model = ClientModelSFL(full_model_peft, split_layer)
    server_model = ServerModelSFL(full_model_peft, split_layer)
    
    logging.info("âœ… æ¨¡å‹åˆ†å‰²å®Œæˆã€‚")
    return client_model.to(device), server_model.to(device)


# ============================== æƒé‡å¤„ç†ä¸èšåˆ ==============================
def get_trainable_state_dict(model: nn.Module) -> dict:
    """è·å–æ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°"""
    return {k: v.clone() for k, v in model.state_dict().items() if v.requires_grad}


def set_trainable_state_dict(model: nn.Module, state_dict: dict):
    """è®¾ç½®æ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°"""
    model.load_state_dict(state_dict, strict=False)


def FedAvg(w_list):
    """è”é‚¦å¹³å‡ç®—æ³•"""
    if not w_list: 
        return None
    aggregated_weights = OrderedDict()
    for key in w_list[0].keys():
        aggregated_weights[key] = torch.stack([w[key] for w in w_list]).mean(dim=0)
    return aggregated_weights


# ============================== SFL è®­ç»ƒä¸è¯„ä¼° ==============================
def client_sfl_train(client_id, client_model, server_model, train_loader, current_lr, 
                     class_weights=None, is_first_client_in_round=False):
    """
    å®¢æˆ·ç«¯ SFL è®­ç»ƒå‡½æ•°ï¼ˆæ”¯æŒç±»åˆ«æƒé‡ï¼‰
    
    Args:
        client_id: å®¢æˆ·ç«¯ID
        client_model: å®¢æˆ·ç«¯æ¨¡å‹
        server_model: æœåŠ¡å™¨ç«¯æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        current_lr: å½“å‰å­¦ä¹ ç‡
        class_weights: ç±»åˆ«æƒé‡ï¼ˆç”¨äºå¤„ç†ä¸å¹³è¡¡ï¼‰
        is_first_client_in_round: æ˜¯å¦æ˜¯æœ¬è½®ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯
    """
    client_model.train()
    server_model.train()
    
    trainable_client_params = [p for p in client_model.parameters() if p.requires_grad]
    trainable_server_params = [p for p in server_model.parameters() if p.requires_grad]
    
    # è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€è½®ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯æ‰“å°ï¼‰
    if is_first_client_in_round:
        print("\n" + "="*80)
        print(f"!!! DEBUG: ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯ (ID: {client_id}) çš„è®­ç»ƒå‰æ£€æŸ¥ !!!")
        print(f"    å®¢æˆ·ç«¯æ¨¡å‹æ‰¾åˆ°çš„å¯è®­ç»ƒå‚æ•°ç»„æ•°é‡: {len(trainable_client_params)}")
        print(f"    æœåŠ¡å™¨æ¨¡å‹æ‰¾åˆ°çš„å¯è®­ç»ƒå‚æ•°ç»„æ•°é‡: {len(trainable_server_params)}")
        
        client_param_count = sum(p.numel() for p in trainable_client_params)
        server_param_count = sum(p.numel() for p in trainable_server_params)
        
        print(f"    å®¢æˆ·ç«¯æ¨¡å‹å¯è®­ç»ƒå‚æ•°æ€»æ•°: {client_param_count:,}")
        print(f"    æœåŠ¡å™¨æ¨¡å‹å¯è®­ç»ƒå‚æ•°æ€»æ•°: {server_param_count:,}")

        if not trainable_client_params:
            print("    !!!!!! è­¦å‘Š: å®¢æˆ·ç«¯ä¼˜åŒ–å™¨æ²¡æœ‰éœ€è¦ä¼˜åŒ–çš„å‚æ•° !!!!!!")
        else:
            print("    âœ… OK: å®¢æˆ·ç«¯ä¼˜åŒ–å™¨æœ‰å‚æ•°å¯ä¼˜åŒ–ã€‚")

        if not trainable_server_params:
            print("    !!!!!! è­¦å‘Š: æœåŠ¡å™¨ç«¯ä¼˜åŒ–å™¨æ²¡æœ‰éœ€è¦ä¼˜åŒ–çš„å‚æ•° !!!!!!")
        else:
            print("    âœ… OK: æœåŠ¡å™¨ç«¯ä¼˜åŒ–å™¨æœ‰å‚æ•°å¯ä¼˜åŒ–ã€‚")
        
        if class_weights is not None:
            print(f"    âœ… ä½¿ç”¨ç±»åˆ«æƒé‡: {class_weights.cpu().numpy()}")
        
        print("="*80 + "\n")

    # å¦‚æœæ²¡æœ‰ä»»ä½•å¯è®­ç»ƒå‚æ•°ï¼Œè®­ç»ƒæ˜¯æ— æ„ä¹‰çš„
    if not trainable_client_params and not trainable_server_params:
        logging.error(f"å®¢æˆ·ç«¯ {client_id}: è‡´å‘½é”™è¯¯ - å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ç«¯å‡æœªæ‰¾åˆ°å¯è®­ç»ƒå‚æ•°ã€‚è·³è¿‡è®­ç»ƒã€‚")
        return get_trainable_state_dict(client_model), 0.0

    optimizer_client = AdamW(trainable_client_params, lr=current_lr) if trainable_client_params else None
    optimizer_server = AdamW(trainable_server_params, lr=current_lr) if trainable_server_params else None

    total_loss, num_batches = 0, 0
    for epoch in range(LOCAL_EPOCHS):
        for batch in train_loader:
            batch_on_device = {k: v.to(device) for k, v in batch.items()}
            labels = batch_on_device['labels']
            
            if optimizer_client: optimizer_client.zero_grad()
            if optimizer_server: optimizer_server.zero_grad()

            smashed_data, extended_attention_mask = client_model(
                input_ids=batch_on_device['input_ids'], 
                attention_mask=batch_on_device['attention_mask']
            )
            smashed_data_server = smashed_data.detach().requires_grad_(True)
            
            # â¬‡ï¸â¬‡ï¸ ä¼ å…¥ç±»åˆ«æƒé‡ â¬‡ï¸â¬‡ï¸
            logits, loss = server_model(smashed_data_server, extended_attention_mask, labels, 
                                       class_weights=class_weights)
            
            if loss is None: 
                continue
            
            loss.backward()
            
            # åªæœ‰åœ¨ smashed_data ä¸Šæœ‰æ¢¯åº¦æ—¶æ‰åå‘ä¼ æ’­åˆ°å®¢æˆ·ç«¯
            if smashed_data_server.grad is not None:
                smashed_data.backward(smashed_data_server.grad)
            
            if optimizer_client: optimizer_client.step()
            if optimizer_server: optimizer_server.step()
            
            total_loss += loss.item()
            num_batches += 1
            
    avg_loss = total_loss / num_batches if num_batches > 0 else 0
    logging.info(f"  å®¢æˆ·ç«¯ {client_id} (LR={current_lr:.2e}) è®­ç»ƒå®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")

    return get_trainable_state_dict(client_model), avg_loss


def evaluate_sfl(client_model, server_model, dataloader):
    """
    è¯„ä¼° SFL æ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼Œæ˜¾ç¤ºè¯¦ç»†é¢„æµ‹åˆ†å¸ƒï¼‰
    """
    client_model.eval()
    server_model.eval()
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="è¯„ä¼°SFLæ¨¡å‹", leave=False):
            batch = {k: v.to(device) for k, v in batch.items()}
            smashed_data, extended_attention_mask = client_model(
                input_ids=batch['input_ids'], 
                attention_mask=batch['attention_mask']
            )
            logits, _ = server_model(smashed_data, extended_attention_mask)
            preds = torch.argmax(logits, dim=-1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(batch['labels'].cpu().numpy())
    
    # ========== è¯¦ç»†é¢„æµ‹åˆ†æ ==========
    pred_counts = Counter(all_preds)
    label_counts = Counter(all_labels)
    
    logging.info(f"  ğŸ“Š é¢„æµ‹åˆ†å¸ƒ: ç±»åˆ«0={pred_counts.get(0, 0):>6}, ç±»åˆ«1={pred_counts.get(1, 0):>6}")
    logging.info(f"  ğŸ“Š çœŸå®åˆ†å¸ƒ: ç±»åˆ«0={label_counts[0]:>6}, ç±»åˆ«1={label_counts[1]:>6}")
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    all_preds_np = np.array(all_preds)
    all_labels_np = np.array(all_labels)
    
    correct_0 = sum((all_preds_np == 0) & (all_labels_np == 0))
    correct_1 = sum((all_preds_np == 1) & (all_labels_np == 1))
    total_0 = label_counts[0]
    total_1 = label_counts[1]
    
    acc_0 = correct_0 / total_0 * 100 if total_0 > 0 else 0
    acc_1 = correct_1 / total_1 * 100 if total_1 > 0 else 0
    
    logging.info(f"  ğŸ“Š ç±»åˆ«0å‡†ç¡®ç‡: {correct_0:>6}/{total_0:>6} = {acc_0:>5.2f}%")
    logging.info(f"  ğŸ“Š ç±»åˆ«1å‡†ç¡®ç‡: {correct_1:>6}/{total_1:>6} = {acc_1:>5.2f}%")
    # ===================================
    
    metrics = {'accuracy': accuracy_score(all_labels, all_preds)}
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='weighted', zero_division=0
    )
    metrics.update({'precision': precision, 'recall': recall, 'f1': f1})
    return metrics


# ============================== å…¶ä»–å·¥å…·å‡½æ•° ==============================
def split_data_for_clients(train_dataset, num_clients):
    """å°†æ•°æ®é›†åˆ†å‰²ç»™å¤šä¸ªå®¢æˆ·ç«¯"""
    client_datasets, all_indices = [], list(range(len(train_dataset)))
    random.shuffle(all_indices)
    for i in range(num_clients):
        subset_indices = all_indices[i::num_clients]
        client_datasets.append(Subset(train_dataset, subset_indices))
    return client_datasets


def save_results_to_csv(all_results, filename, config):
    """ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶"""
    try:
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['# Configuration'])
            for k, v in config.items():
                writer.writerow([f'# {k}', v])
            writer.writerow([])
            
            fieldnames = ['round', 'loss', 'accuracy', 'f1', 'precision', 'recall']
            writer.writerow(fieldnames)
            
            for result in all_results:
                writer.writerow([
                    result['round'], 
                    f"{result['loss']:.4f}", 
                    f"{result['accuracy']:.4f}", 
                    f"{result['f1']:.4f}", 
                    f"{result['precision']:.4f}", 
                    f"{result['recall']:.4f}"
                ])
        logging.info(f"âœ… SFLè¯„ä¼°ç»“æœå·²æˆåŠŸä¿å­˜åˆ°æ–‡ä»¶: {filename}")
    except Exception as e:
        logging.error(f"ä¿å­˜SFLç»“æœæ–‡ä»¶å¤±è´¥: {e}")


def print_trainable_parameters_manually(model: nn.Module):
    """æ‰‹åŠ¨æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
    if hasattr(model, 'print_trainable_parameters'):
        model.print_trainable_parameters()
        return

    trainable_params, all_param = 0, 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    
    percentage = 100 * trainable_params / all_param if all_param > 0 else 0
    logging.info(f"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {percentage:.4f}%")


def verify_local_model(model_path):
    """éªŒè¯æœ¬åœ°æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´"""
    print("\n" + "="*70)
    print("ğŸ” æ­£åœ¨éªŒè¯æœ¬åœ°æ¨¡å‹...")
    
    if not os.path.exists(model_path):
        logging.error(f"âŒ æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        logging.error("è¯·å…ˆè¿è¡Œ download_roberta.py ä¸‹è½½æ¨¡å‹")
        return False
    
    required_files = ['config.json', 'pytorch_model.bin', 'tokenizer.json', 'vocab.json', 'merges.txt']
    missing_files = []
    
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if not os.path.exists(file_path):
            missing_files.append(file)
        else:
            size_mb = os.path.getsize(file_path) / (1024 * 1024)
            logging.info(f"  âœ… {file:<25} ({size_mb:>8.2f} MB)")
    
    if missing_files:
        logging.error(f"âŒ ç¼ºå¤±å…³é”®æ–‡ä»¶: {missing_files}")
        return False
    
    logging.info(f"âœ… æœ¬åœ°æ¨¡å‹éªŒè¯é€šè¿‡: {model_path}")
    print("="*70 + "\n")
    return True


# ============================== ä¸»å‡½æ•° ==============================
def main():
    print("="*70)
    print("åˆ†å‰²è”é‚¦å­¦ä¹  (SFL): ä½¿ç”¨ LoRA å¾®è°ƒ RoBERTa æ¨¡å‹")
    print(f"å½“å‰æ¨¡å¼: {MODE}")
    print(f"æ”¹è¿›: å¢å¼ºè®­ç»ƒå¼ºåº¦ + ç±»åˆ«æƒé‡ + è¯¦ç»†è¯„ä¼°")
    print("="*70 + "\n")
    
    # ==================== éªŒè¯æœ¬åœ°æ¨¡å‹ ====================
    if not verify_local_model(MODEL_NAME):
        logging.error("æ¨¡å‹éªŒè¯å¤±è´¥ï¼Œç¨‹åºé€€å‡ºã€‚")
        return
    
    # ==================== è‡ªåŠ¨æ•°æ®æ£€æŸ¥ä¸å¤„ç† ====================
    logging.info("æ­¥éª¤ 0: æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    
    train_file = 'processed_data/train_data.jsonl'
    test_file = 'processed_data/test_data.jsonl'
    
    data_exists = os.path.exists(train_file) and os.path.exists(test_file)
    
    if not data_exists:
        logging.warning("âš ï¸  æœªæ‰¾åˆ°å·²å¤„ç†çš„æ•°æ®æ–‡ä»¶ï¼Œå°è¯•è‡ªåŠ¨å¤„ç†...")
        
        try:
            import UNSW_NB15_processed_llm as data_processor
            
            if MODE == 'QUICK':
                debug_rows = 10000
                logging.info(f"QUICK æ¨¡å¼ï¼šä½¿ç”¨ {debug_rows} è¡Œæ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•")
            else:
                debug_rows = None
                logging.info("NORMAL æ¨¡å¼ï¼šä½¿ç”¨å®Œæ•´æ•°æ®é›†")
            
            success = data_processor.check_and_prepare_data(
                data_path='data/UNSW-NB15.csv',
                debug_rows=debug_rows,
                force_reprocess=False
            )
            
            if not success:
                logging.error("âŒ æ•°æ®å¤„ç†å¤±è´¥ï¼Œç¨‹åºé€€å‡ºã€‚")
                return
                
        except ImportError:
            logging.error("âŒ æ— æ³•å¯¼å…¥æ•°æ®å¤„ç†æ¨¡å— 'UNSW_NB15_processed_llm.py'")
            return
        except Exception as e:
            logging.error(f"âŒ æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return
    else:
        logging.info(f"âœ… å‘ç°å·²å¤„ç†çš„æ•°æ®æ–‡ä»¶")
    
    # ==================== é…ç½®ä¿¡æ¯ ====================
    training_config = {
        "Mode": MODE,
        "Framework": "Split Federated Learning",
        "Model": MODEL_NAME_ON_HUB,
        "Model_Path": MODEL_NAME,
        "Split_Layer": SPLIT_LAYER,
        "LoRA_Rank_(r)": LORA_R,
        "LoRA_Alpha_(alpha)": LORA_ALPHA,
        "Communication_Rounds": ROUNDS,
        "Clients_per_Round": CLIENTS_PER_ROUND,
        "Total_Clients": NUM_CLIENTS,
        "Local_Epochs": LOCAL_EPOCHS,
        "Learning_Rate_(LR)": LR
    }

    # ==================== åŠ è½½æ•°æ® ====================
    logging.info("æ­¥éª¤ 1: åŠ è½½æ•°æ®å¹¶åˆ’åˆ†...")
    
    tokenizer = get_tokenizer(MODEL_NAME)
    
    full_train_dataset = FT_Dataset(train_file, BATCH_SIZE, MAX_SEQ_LENGTH, tokenizer)
    test_dataset = FT_Dataset(test_file, BATCH_SIZE, MAX_SEQ_LENGTH, tokenizer)
    
    # ========== è®¡ç®—ç±»åˆ«æƒé‡ ==========
    logging.info("æ­£åœ¨è®¡ç®—ç±»åˆ«æƒé‡...")
    all_labels = []
    for i in range(len(full_train_dataset)):
        all_labels.append(full_train_dataset[i]['labels'].item())
    
    label_counts = Counter(all_labels)
    logging.info(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: ç±»åˆ«0={label_counts[0]}, ç±»åˆ«1={label_counts[1]}")
    
    # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆåæ¯”äºç±»åˆ«é¢‘ç‡ï¼‰
    total_samples = len(all_labels)
    class_weights = torch.tensor([
        total_samples / (2 * label_counts[0]),  # ç±»åˆ«0çš„æƒé‡
        total_samples / (2 * label_counts[1])   # ç±»åˆ«1çš„æƒé‡
    ], dtype=torch.float32).to(device)
    
    logging.info(f"ç±»åˆ«æƒé‡: ç±»åˆ«0={class_weights[0]:.3f}, ç±»åˆ«1={class_weights[1]:.3f}")
    logging.info("ï¼ˆæƒé‡è¶Šå¤§ï¼Œè¯¥ç±»åˆ«åœ¨æŸå¤±è®¡ç®—ä¸­è¶Šé‡è¦ï¼‰")
    # ====================================
    
    if DEBUG_DATA_SIZE is not None and DEBUG_DATA_SIZE < len(full_train_dataset):
        indices = torch.randperm(len(full_train_dataset))[:DEBUG_DATA_SIZE]
        train_subset = Subset(full_train_dataset, indices.tolist())
        client_datasets = split_data_for_clients(train_subset, NUM_CLIENTS)
    else:
        client_datasets = split_data_for_clients(full_train_dataset, NUM_CLIENTS)
    
    client_loaders = [DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True) for ds in client_datasets]
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # ==================== åˆå§‹åŒ–æ¨¡å‹ ====================
    logging.info("æ­¥éª¤ 2: åˆå§‹åŒ–å…¨å±€æ¨¡å‹ä¸å…¨å±€è°ƒåº¦å™¨...")
    
    global_client_model, global_server_model = create_and_split_model(SPLIT_LAYER, MODEL_NAME)
    
    logging.info("--- (æ‰‹åŠ¨æ£€æŸ¥) å®¢æˆ·ç«¯å¯è®­ç»ƒå‚æ•° ---")
    print_trainable_parameters_manually(global_client_model)
    logging.info("--- (æ‰‹åŠ¨æ£€æŸ¥) æœåŠ¡å™¨ç«¯å¯è®­ç»ƒå‚æ•° ---")
    print_trainable_parameters_manually(global_server_model)

    dummy_optimizer = AdamW([torch.zeros(1)], lr=LR)
    global_scheduler = get_linear_schedule_with_warmup(
        dummy_optimizer, 
        num_warmup_steps=0,
        num_training_steps=ROUNDS
    )

    # ==================== å¼€å§‹è®­ç»ƒ ====================
    logging.info("æ­¥éª¤ 3: å¼€å§‹ SFL è®­ç»ƒ...")
    all_round_results = []
    
    for round_num in range(1, ROUNDS + 1):
        logging.info(f"\n{'='*70}")
        logging.info(f"é€šä¿¡è½®æ¬¡ {round_num}/{ROUNDS}")
        logging.info("="*70)
        
        selected_client_ids = random.sample(range(NUM_CLIENTS), CLIENTS_PER_ROUND)
        logging.info(f"æœ¬è½®å‚ä¸å®¢æˆ·ç«¯: {selected_client_ids}")
        
        current_round_lr = global_scheduler.get_last_lr()[0]
        logging.info(f"å½“å‰å­¦ä¹ ç‡: {current_round_lr:.2e}")
        
        round_server_model = copy.deepcopy(global_server_model)
        
        local_client_weights, local_losses = [], []
        
        for i, client_id in enumerate(selected_client_ids):
            local_client_model = copy.deepcopy(global_client_model)
            
            # åªåœ¨å…¨å±€çš„ç¬¬ä¸€è½®ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯æ‰“å°è°ƒè¯•ä¿¡æ¯
            is_first = (i == 0 and round_num == 1)
            
            # â¬‡ï¸â¬‡ï¸ ä¼ å…¥ç±»åˆ«æƒé‡ â¬‡ï¸â¬‡ï¸
            client_w, loss = client_sfl_train(
                client_id, 
                local_client_model, 
                round_server_model, 
                client_loaders[client_id], 
                current_round_lr,
                class_weights=class_weights,  # â¬…ï¸ å…³é”®ï¼šä¼ å…¥ç±»åˆ«æƒé‡
                is_first_client_in_round=is_first
            )
            local_client_weights.append(client_w)
            local_losses.append(loss)
            
        # èšåˆå®¢æˆ·ç«¯æƒé‡
        global_client_weights = FedAvg(local_client_weights)
        if global_client_weights: 
            set_trainable_state_dict(global_client_model, global_client_weights)
        
        # æ›´æ–°å…¨å±€æœåŠ¡å™¨æ¨¡å‹
        final_round_server_weights = get_trainable_state_dict(round_server_model)
        set_trainable_state_dict(global_server_model, final_round_server_weights)
        
        logging.info(f"æœåŠ¡å™¨èšåˆå®Œæˆï¼Œå…¨å±€æ¨¡å‹å·²æ›´æ–°ã€‚")
        
        # è¯„ä¼°
        logging.info("æ­£åœ¨è¯„ä¼°...")
        metrics = evaluate_sfl(global_client_model, global_server_model, test_loader)
        avg_round_loss = np.mean(local_losses)
        round_result = {'round': round_num, 'loss': avg_round_loss, **metrics}
        all_round_results.append(round_result)
        
        global_scheduler.step()
        
        # æ‰“å°è½®æ¬¡æ€»ç»“
        print()
        logging.info(f"{'='*70}")
        logging.info(f"è½®æ¬¡ {round_num} æ€»ç»“")
        logging.info("="*70)
        logging.info(f"  å¹³å‡è®­ç»ƒæŸå¤±: {avg_round_loss:.4f}")
        logging.info(f"  æµ‹è¯•é›† å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
        logging.info(f"  æµ‹è¯•é›† F1 åˆ†æ•°: {metrics['f1']:.4f}")
        logging.info(f"  æµ‹è¯•é›† ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
        logging.info(f"  æµ‹è¯•é›† å¬å›ç‡: {metrics['recall']:.4f}")
        logging.info("="*70)
        print()

    # ==================== ä¿å­˜ç»“æœ ====================
    logging.info("æ­¥éª¤ 4: è®­ç»ƒå®Œæˆï¼Œä¿å­˜ç»“æœ...")
    save_results_to_csv(all_round_results, RESULTS_FILENAME, training_config)
    
    print("\n" + "="*70)
    logging.info("âœ… SFL è®­ç»ƒå®Œæˆï¼")
    logging.info(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {RESULTS_FILENAME}")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
